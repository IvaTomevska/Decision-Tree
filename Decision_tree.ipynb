{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "import csv\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from numpy import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass classifier decision tree using ID3 algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize the entire dataset prior to learning using min-max normalization \n",
    "def normalize(matrix):\n",
    "    a=np.array(matrix)\n",
    "    a=a.astype(np.float)\n",
    "    #print(\"Before normalizing\")\n",
    "    #print(a)\n",
    "    b = np.apply_along_axis(lambda x: (x-np.min(x))/float(np.max(x)-np.min(x)),0,a)\n",
    "    #print(b)\n",
    "    return b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading from the file using numpy genfromtxt with delimiter ','\n",
    "def load_csv(file):\n",
    "    X = genfromtxt(file, delimiter=\",\",dtype=str)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to randomly shuffle the array using the numpy.random.shuffle()\n",
    "def random_numpy_array(ar):\n",
    "    np.random.shuffle(ar)\n",
    "    arr = ar\n",
    "    #print(arr)\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data and generate the training labels,training features, test labels and test training\n",
    "def generate_set(X):\n",
    "    #print(X.shape[0])\n",
    "    Y = X[:,-1]\n",
    "    j = Y.reshape(len(Y),1)\n",
    "    #print(\"J is\",j)\n",
    "    \n",
    "    new_X = X[:,:-1]\n",
    "    #normalize the data step\n",
    "    normalized_X = normalize(new_X)\n",
    "    X = np.concatenate((normalized_X,j),axis=1)\n",
    "    \n",
    "    rows_size = X.shape[0]\n",
    "    num_test = round(0.1*rows_size)\n",
    "    \n",
    "    start = 0\n",
    "    end = num_test\n",
    "    \n",
    "    test_features =[]\n",
    "    test_labels =[]\n",
    "    train_features = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        X_test = X[start:end , :]\n",
    "        before = X[:start, :]\n",
    "        after = X[end:, :]\n",
    "        X_train = np.concatenate((before, after), axis=0)\n",
    "\n",
    "        #print(\"Before normalizing\",X_test)\n",
    "        y_test = X_test[:, -1].flatten()\n",
    "        y_train = X_train[:,-1].flatten()\n",
    "\n",
    "        X_test = X_test[:,:-1]\n",
    "        X_train = X_train[:,:-1]\n",
    "        X_test = X_test.astype(np.float)\n",
    "        X_train = X_train.astype(np.float)\n",
    "        test_features.append(X_test)\n",
    "        test_labels.append(y_test)\n",
    "        train_features.append(X_train)\n",
    "        train_labels.append(y_train)\n",
    "        #print(\"start: \",start)\n",
    "        #print(\"end: \",end)\n",
    "        start = end\n",
    "        end = end+num_test\n",
    "    return test_features,test_labels,train_features,train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a dictionary where the key is the class label and values are the features which belong to that class.\n",
    "def build_dict_of_attributes_with_class_values(X,y):\n",
    "    \n",
    "    dict_of_attri = {}\n",
    "    fea_list =[]\n",
    "    \n",
    "    for i in range(X.shape[1]):\n",
    "        fea_index = i\n",
    "        all_values = X[:,i]\n",
    "        #print(\"all_values: \"all_values)\n",
    "        attribute_list =[]\n",
    "\n",
    "        for index, j in enumerate(all_values):\n",
    "            attribute_value = []\n",
    "            attribute_value.append(j)\n",
    "            attribute_value.append(y[index])\n",
    "            attribute_list.append(attribute_value)\n",
    "\n",
    "        dict_of_attri[fea_index]= attribute_list\n",
    "        fea_list.append(fea_index)\n",
    "    \n",
    "    return dict_of_attri,fea_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterative Dichotomiser 3 entropy calculation\n",
    "def entropy(y):\n",
    "    class_freq = {}\n",
    "    attri_entropy = 0.0\n",
    "    \n",
    "    for i in y:\n",
    "        if i in class_freq:\n",
    "            class_freq[i] += 1\n",
    "        else:\n",
    "            class_freq[i] = 1\n",
    "    #print(class_freq)\n",
    "    \n",
    "    for freq in class_freq.values():\n",
    "        attri_entropy += (-freq/float(len(y))) * math.log(freq/float(len(y)),2)\n",
    "    #print(attri_entropy)\n",
    "    \n",
    "    return attri_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class node and explanation is self explaination\n",
    "class Node(object):\n",
    "#     init the node with val,lchild,rchild,thea and leaf.\n",
    "    def __init__(self, val, lchild, rchild,thea,leaf):\n",
    "        self.root_value = val\n",
    "        self.root_left = lchild\n",
    "        self.root_right = rchild\n",
    "        self.theta = thea\n",
    "        self.leaf = leaf\n",
    "\n",
    "#     method to identify if the node is leaf\n",
    "    def is_leaf(self):\n",
    "        \n",
    "        return self.leat\n",
    "\n",
    "#     method to return threshold value\n",
    "    def ret_thetha(self):\n",
    "        \n",
    "        return self.theta\n",
    "    \n",
    "#     method return root value\n",
    "    def ret_root_value(self):\n",
    "        \n",
    "        return root_value\n",
    "    \n",
    "#     method return left tree\n",
    "    def ret_llist(self):\n",
    "        \n",
    "        return root_left\n",
    "\n",
    "#     method return right tree\n",
    "    def ret_rlist(self):\n",
    "        \n",
    "        return root_right\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"(%r, %r, %r, %r)\" %(self.root_value,self.root_left,self.root_right,self.theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree object\n",
    "class DecisionTree(object):\n",
    "    fea_list = []\n",
    "    def __init__(self):\n",
    "        self.root_node = None\n",
    "\n",
    "    #method to return the major class value\n",
    "    def cal_major_class_values(self,class_values):\n",
    "        data = Counter(class_values).most_common(1)[0][0]\n",
    "        return data\n",
    "\n",
    "    #method to calculate best threshold value for each feature\n",
    "    def cal_best_theta_value(self,ke,attri_list):\n",
    "        data = []\n",
    "        class_labels = []\n",
    "        \n",
    "        for i in attri_list:\n",
    "            data.append(i[0])\n",
    "            class_labels.append(i[1])\n",
    "            \n",
    "        feature_values_entropy = entropy(class_labels)\n",
    "\n",
    "        max_info_gain = 0\n",
    "        theta = 0\n",
    "        \n",
    "        best__left_index = []\n",
    "        best_right_index = []\n",
    "        after_split = []\n",
    "        \n",
    "        data.sort()\n",
    "        \n",
    "        for i in range(len(data) - 1):\n",
    "            cur_theta = float(data[i]+data[i+1])/ 2\n",
    "            index_less_theta = []\n",
    "            values_less_theta = []\n",
    "            index_greater_theta = []\n",
    "            values_greater_theta = []\n",
    "\n",
    "            for c,j in enumerate(attri_list):\n",
    "                #print(c,j[0])\n",
    "                if j[0] <= cur_theta:\n",
    "                    values_less_theta.append(j[1])\n",
    "                    index_less_theta.append(c)\n",
    "                else:\n",
    "                    values_greater_theta.append(j[1])\n",
    "                    index_greater_theta.append(c)\n",
    "\n",
    "            entropy_of_less = entropy(values_less_theta)\n",
    "            #print(entropy_of_less)\n",
    "            entropy_of_greater = entropy(values_greater_theta)\n",
    "            #print(entropy_of_greater)\n",
    "\n",
    "            info_gain = feature_values_entropy - (entropy_of_less*(len(index_less_theta)/(len(data)))) \\\n",
    "                            - (entropy_of_greater*(len(index_greater_theta)/(len(data))))\n",
    "\n",
    "            if info_gain > max_info_gain:\n",
    "                max_info_gain = info_gain\n",
    "                theta = cur_theta\n",
    "                best_left_index = index_less_theta\n",
    "                best_right_index = index_greater_theta\n",
    "                after_split = values_less_theta + values_greater_theta\n",
    "        \n",
    "        return max_info_gain, theta, best_left_index, best_right_index, after_split\n",
    "\n",
    "    #method to select the best feature out of all the features.\n",
    "    def best_feature(self,dict_rep):\n",
    "        \n",
    "        key_value = None\n",
    "        best_info_gain = -1\n",
    "        best_theta = 0\n",
    "        best_left = []\n",
    "        best_right = []\n",
    "        best_class = []\n",
    "        result_list = []\n",
    "        for ke in dict_rep.keys():\n",
    "            #print(\"Key \",ke)\n",
    "            info_gain, theta, index_left, index_right, after_split = self.cal_best_theta_value(ke,dict_rep[ke])\n",
    "            #print(\"Best theta \", ke, info_gain, theta, index_left)\n",
    "            if info_gain > best_info_gain:\n",
    "                best_info_gain = info_gain\n",
    "                best_theta = theta\n",
    "                key_value = ke\n",
    "                best_left = index_left\n",
    "                best_right = index_right\n",
    "                best_class = after_split\n",
    "        \n",
    "        result_list.append(key_value)\n",
    "        result_list.append(best_theta)\n",
    "        result_list.append(best_left)\n",
    "        result_list.append(best_right)\n",
    "        result_list.append(best_class)\n",
    "        \n",
    "        return result_list\n",
    "\n",
    "    def get_remainder_dict(self,dict_of_everything,index_split):\n",
    "        global fea_list\n",
    "        split_dict = {}\n",
    "\n",
    "        for ke in dict_of_everything.keys():\n",
    "            val_list = []\n",
    "            modified_list = []\n",
    "            cor_val = dict_of_everything[ke]\n",
    "            \n",
    "            for index,val in enumerate(cor_val):\n",
    "                \n",
    "                if index not in index_split:\n",
    "                    modified_list.append(val)\n",
    "                    val_list.append(val[1])\n",
    "            #print(modified_list)\n",
    "            \n",
    "            split_dict[ke] = modified_list\n",
    "\n",
    "        return split_dict,val_list\n",
    "\n",
    "    #method to create decision tree\n",
    "    def create_decision_tree(self, dict_of_everything,class_val,eta_min_val):\n",
    "        global fea_list\n",
    "\n",
    "        #if all the class labels are same, then we are set\n",
    "        if len(set(class_val)) ==1:\n",
    "            root_node =  Node(class_val[0],None,None,0,True)\n",
    "            return root_node\n",
    "        \n",
    "        #if the no class vales are less than threshold, we assign the class with max values as the class label    \n",
    "        elif len(class_val) < eta_min_val:\n",
    "            majority_val = self.cal_major_class_values(class_val)\n",
    "            root_node = Node(majority_val,None,None,0,True)\n",
    "            return root_node\n",
    "\n",
    "        else:\n",
    "            best_features_list = self.best_feature(dict_of_everything)\n",
    "            #print(best_features_list)\n",
    "            node_name = best_features_list[0]\n",
    "            theta = best_features_list[1]\n",
    "            left_split = best_features_list[2]\n",
    "            right_split = best_features_list[3]\n",
    "            class_labels = best_features_list[4]\n",
    "            \n",
    "            left_tree,left_val = self.get_remainder_dict(dict_of_everything,left_split)\n",
    "            right_tree,right_val = self.get_remainder_dict(dict_of_everything,right_split)\n",
    "            \n",
    "            left_child = self.create_decision_tree(left_tree,left_val,eta_min_val)\n",
    "            right_child = self.create_decision_tree(right_tree,right_val,eta_min_val)\n",
    "            \n",
    "            root_node = Node(node_name,right_child,left_child,theta,False)\n",
    "            \n",
    "            return root_node\n",
    "\n",
    "            \n",
    "\n",
    "    def fit(self, dict_of_everything,cl_val,features,eta_min_val):\n",
    "        global fea_list\n",
    "        fea_list = features\n",
    "        root_node = self.create_decision_tree(dict_of_everything,cl_val,eta_min_val)\n",
    "        \n",
    "        return root_node\n",
    "\n",
    "    def classify(self,row,root):\n",
    "        test_dict ={}\n",
    "        \n",
    "        for k,j in enumerate(row):\n",
    "            test_dict[k] = j\n",
    "        #print(test_dict)\n",
    "        \n",
    "        current_node = root\n",
    "        \n",
    "        while not current_node.leaf:\n",
    "            #print(current_node.root_value,test_dict[current_node.root_value], current_node.theta)\n",
    "            if test_dict[current_node.root_value] <= current_node.theta:\n",
    "                current_node = current_node.root_left\n",
    "            else:\n",
    "                current_node = current_node.root_right\n",
    "        #print(current_node.root_value,test_dict[current_node.root_value], current_node.theta)\n",
    "        return current_node.root_value\n",
    "        \n",
    "    #method to the labels for the test data\n",
    "    def predict(self, X, root):\n",
    "        predict_list = []\n",
    "        for row in X:\n",
    "            y_pred = self.classify(row,root)\n",
    "            predict_list.append(y_pred)\n",
    "        return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the predicited accuracy\n",
    "def accuracy_for_predicted_values(test_class_names1,l):\n",
    "    true_ctr = 0\n",
    "    false_ctr = 0\n",
    "    \n",
    "    for i in range(len(test_class_names1)):\n",
    "        if(test_class_names1[i] == l[i]):\n",
    "            true_ctr += 1\n",
    "        else:\n",
    "            false_ctr += 1\n",
    "    \n",
    "    return true_ctr, false_ctr, float(true_ctr)/len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(num_arr, eta_min):\n",
    "    eta_min_val = round(eta_min*num_arr.shape[0])\n",
    "    sh_arr = random_numpy_array(num_arr)\n",
    "    test_features, test_labels, train_features, train_labels = generate_set(sh_arr)\n",
    "    cumulate_acc = 0\n",
    "    \n",
    "    standard_dev_set = []\n",
    "    sum_of_squares = 0.0\n",
    "    \n",
    "    #ten fold iteration \n",
    "    for i in range(10):\n",
    "        dict_labels,fea = build_dict_of_attributes_with_class_values(train_features[i], train_labels[i])\n",
    "        decision_tree = DecisionTree()\n",
    "        decision_tree_model = decision_tree.fit(dict_labels,train_labels[i],fea,eta_min_val)\n",
    "        pred_labels = decision_tree.predict(test_features[i], decision_tree_model)\n",
    "    \n",
    "        right,wrong,accu = accuracy_for_predicted_values(test_labels[i], pred_labels)\n",
    "\n",
    "        cumulate_acc += accu\n",
    "        \n",
    "        standard_dev_set.append(accu)      \n",
    "        standard_mean = np.mean(standard_dev_set)\n",
    "        \n",
    "        for i in standard_dev_set:\n",
    "            sum_of_squares += (i-standard_mean)**2\n",
    "        \n",
    "        final_deviation = np.sqrt(sum_of_squares/len(standard_dev_set))\n",
    "        \n",
    "        \n",
    "        print(\"Accuracy is \",accu)\n",
    "    print(\"Accuracy across 10-cross validation for\",eta_min,\"is\",float(cumulate_acc)/10)\n",
    "    print(\"Standard deviation is \", final_deviation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  0.8\n",
      "Accuracy is  1.0\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy across 10-cross validation for 0.05 is 0.9266666666666667\n",
      "Standard deviation is  0.14955076763014433\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy across 10-cross validation for 0.1 is 0.9466666666666667\n",
      "Standard deviation is  0.11960620276129942\n",
      "Accuracy is  1.0\n",
      "Accuracy is  1.0\n",
      "Accuracy is  1.0\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.8\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy across 10-cross validation for 0.15 is 0.9533333333333334\n",
      "Standard deviation is  0.14288597592626825\n",
      "Accuracy is  1.0\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.8666666666666667\n",
      "Accuracy is  1.0\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy is  0.9333333333333333\n",
      "Accuracy across 10-cross validation for 0.2 is 0.9466666666666667\n",
      "Standard deviation is  0.1419118637475754\n"
     ]
    }
   ],
   "source": [
    "eta_min_list = [0.05,0.10,0.15,0.20]\n",
    "newfile = \"iris.csv\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "sambase.csv not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-dbefed509204>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0meta_min_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnewfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sambase.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnum_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meta_min_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_arr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-8b5861e15289>\u001b[0m in \u001b[0;36mload_csv\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# reading from the file using numpy genfromtxt with delimiter ','\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1770\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1772\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1773\u001b[0m             \u001b[0mfid_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: sambase.csv not found."
     ]
    }
   ],
   "source": [
    "eta_min_list = [0.05,0.10,0.15,0.20,0.25]\n",
    "newfile = \"sambase.csv\"\n",
    "num_arr = load_csv(newfile)\n",
    "for i in eta_min_list:\n",
    "    main(num_arr,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
